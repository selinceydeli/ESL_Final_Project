{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elements of Statistical Learning (ESL) Final Project\n",
    "\n",
    "**Our Machine Learning Question:**\n",
    "\n",
    "**How can we improve the accuracy of predicting** which diabetic patients are likely to be readmitted to the hospital within 30 days across different classifiers **by addressing the severe class imbalance in the dataset?**\n",
    "The dataset we chose is on “Early Readmission Prediction of Patients Diagnosed with Diabetes,” which has a significant class imbalance between early admitted patients and others. The positive class for our classification (early admitted patients) is underrepresented with a ratio of 1-to-9.\n",
    "\n",
    "The techniques we are considering implementing to overcome the class imbalance in the dataset are as follows:\n",
    "1. SMOTE (Synthetic Minority Oversampling Technique)\n",
    "2. Class weights\n",
    "3. Ensemble methods\n",
    "\n",
    "All the above techniques will be compared across different classifiers to answer the following question: How well do different models (e.g., Logistic Regression, Random Forest, SVM) handle the imbalance and predict early readmissions?\n",
    "\n",
    "Recommendations from Professor David:\n",
    "\n",
    "- Check how different models react to class imbalance prior to implementing balancing techniques. \n",
    "- Also, check if the balancing techniques actually cause that the same objects are misclassified, or that suddenly also other objects go wrong (that used to be classified well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Exploration\n",
    "\n",
    "The chosen dataset represents ten years (1999-2008) of clinical care at 130 US hospitals and integrated delivery networks. Each row concerns hospital records of patients diagnosed with diabetes, who underwent laboratory, medications, and stayed up to 14 days.\n",
    "\n",
    "Link to the dataset: https://archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008\n",
    "\n",
    "**Our Aim:** Identifying patients at risk of being readmitted within 30 days of discharge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   encounter_id  patient_nbr             race  gender      age weight  \\\n",
      "0       2278392      8222157        Caucasian  Female   [0-10)      ?   \n",
      "1        149190     55629189        Caucasian  Female  [10-20)      ?   \n",
      "2         64410     86047875  AfricanAmerican  Female  [20-30)      ?   \n",
      "3        500364     82442376        Caucasian    Male  [30-40)      ?   \n",
      "4         16680     42519267        Caucasian    Male  [40-50)      ?   \n",
      "\n",
      "   admission_type_id  discharge_disposition_id  admission_source_id  \\\n",
      "0                  6                        25                    1   \n",
      "1                  1                         1                    7   \n",
      "2                  1                         1                    7   \n",
      "3                  1                         1                    7   \n",
      "4                  1                         1                    7   \n",
      "\n",
      "   time_in_hospital  ... citoglipton insulin  glyburide-metformin  \\\n",
      "0                 1  ...          No      No                   No   \n",
      "1                 3  ...          No      Up                   No   \n",
      "2                 2  ...          No      No                   No   \n",
      "3                 2  ...          No      Up                   No   \n",
      "4                 1  ...          No  Steady                   No   \n",
      "\n",
      "   glipizide-metformin  glimepiride-pioglitazone  metformin-rosiglitazone  \\\n",
      "0                   No                        No                       No   \n",
      "1                   No                        No                       No   \n",
      "2                   No                        No                       No   \n",
      "3                   No                        No                       No   \n",
      "4                   No                        No                       No   \n",
      "\n",
      "   metformin-pioglitazone  change diabetesMed readmitted  \n",
      "0                      No      No          No         NO  \n",
      "1                      No      Ch         Yes        >30  \n",
      "2                      No      No         Yes         NO  \n",
      "3                      No      Ch         Yes         NO  \n",
      "4                      No      Ch         Yes         NO  \n",
      "\n",
      "[5 rows x 50 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('diabetic_data.csv')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['encounter_id', 'patient_nbr', 'race', 'gender', 'age', 'weight',\n",
      "       'admission_type_id', 'discharge_disposition_id', 'admission_source_id',\n",
      "       'time_in_hospital', 'payer_code', 'medical_specialty',\n",
      "       'num_lab_procedures', 'num_procedures', 'num_medications',\n",
      "       'number_outpatient', 'number_emergency', 'number_inpatient', 'diag_1',\n",
      "       'diag_2', 'diag_3', 'number_diagnoses', 'max_glu_serum', 'A1Cresult',\n",
      "       'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide',\n",
      "       'glimepiride', 'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide',\n",
      "       'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone',\n",
      "       'tolazamide', 'examide', 'citoglipton', 'insulin',\n",
      "       'glyburide-metformin', 'glipizide-metformin',\n",
      "       'glimepiride-pioglitazone', 'metformin-rosiglitazone',\n",
      "       'metformin-pioglitazone', 'change', 'diabetesMed', 'readmitted'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of 'readmitted' target column:\n",
      "readmitted\n",
      "NO     54864\n",
      ">30    35545\n",
      "<30    11357\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentage distribution of 'readmitted' target column:\n",
      "readmitted\n",
      "NO     53.91\n",
      ">30    34.93\n",
      "<30    11.16\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "target_distribution = data['readmitted'].value_counts()\n",
    "\n",
    "print(\"Distribution of 'readmitted' target column:\")\n",
    "print(target_distribution)\n",
    "\n",
    "target_percentage = data['readmitted'].value_counts(normalize=True) * 100\n",
    "print(\"\\nPercentage distribution of 'readmitted' target column:\")\n",
    "print(f\"{round(target_percentage, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification Formulation\n",
    "\n",
    "There are three classes in the target 'readmitted' column:\n",
    "- Class 1: <30 (early admission)\n",
    "- Class 2: NO (no admission)\n",
    "- Class 3: >30 (late admission)\n",
    "\n",
    "For the purpose of our early admission prediction problem, we will use binary classification because we are looking for a way to classify early admissions correctly in this problem. Therefore, for our problem, the \"No\" and \">30\" cases mean the same and can be combined into the same group.\n",
    "\n",
    "Hence, the **binary classification** formulation can be described as follows:\n",
    "- Class 1: <30 (early admission)\n",
    "- Class 2: NO and >30 (no admission and late admission)\n",
    "\n",
    "### Understanding the Imbalance in the Dataset\n",
    "\n",
    "After formulating the dataset as a binary classification problem, it can be seen that the combined percentage of the \"No\" and \">30\" classes is approximately 88.84%, whereas the percentage distribution of the early admission class \"<30\" is 11.16%.\n",
    "\n",
    "Hence, it can be seen that there is a severe imbalance in the dataset. The positive class for our classification (early admitted patients) is underrepresented, with a ratio of 1-to-9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO During the Christmas Break\n",
    "\n",
    "Link to the Google sheets for writing down comments on the dataset columns and rationalizing the implemented approaches:\n",
    "https://docs.google.com/spreadsheets/d/1wQvVQijqFmdOWjL4nJtSQ5hRea5tVSDQo9-br04bgew/edit?usp=sharing\n",
    "\n",
    "## Data Exploration Ideas\n",
    "\n",
    "### 1. Missing Value Handling\n",
    "- **Indicator of Missing Values**: Missing values are represented by `?` in the dataset.\n",
    "- **Imputation Strategies**:\n",
    "  - **Baseline Approach: Deleting the Rows with Missing Values:** Remove rows that contain at least one missing value in any column.\n",
    "  - **Correlation-based Imputation**: Identify which features are highly correlated and impute missing values accordingly.\n",
    "  - **Statistical Methods**:\n",
    "    - For **categorical features**: Use the most frequent class (mode).\n",
    "    - For **numerical features**: Use the mean, median, or a neighbor-based approach (e.g., K-Nearest Neighbors).\n",
    "      - **Note**: Research the name of the neighbor-based imputation technique (e.g., KNN Imputation).\n",
    "- **Documentation**: Record the rationale behind the chosen imputation strategy.\n",
    "\n",
    "### 2. Feature Reduction & Extraction\n",
    "- **Initial Cleanup**:\n",
    "  - Drop non-informative columns such as:\n",
    "    - ID fields.\n",
    "    - Columns with constant values (e.g., same value for all rows).\n",
    "- **Rationale**: Clearly document why specific columns were dropped.\n",
    "\n",
    "- **Dimensionality Reduction**:\n",
    "  - Implement methods like **Principal Component Analysis (PCA)**.\n",
    "  - **Library**: Check if Scikit-learn has built-in functions for feature extraction or reduction.\n",
    "  - Alternatively, develop custom logic for feature reduction based on feature correlation.\n",
    "- **Rationale**: Justify the use of any dimensionality reduction method and its impact on the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Preprocessing Ideas\n",
    "\n",
    "### 1. Normalization\n",
    "- **Research Questions**:\n",
    "  - Which features require normalization?\n",
    "    - Should normalization apply only to the target class or to all features?\n",
    "- **Initial Assumption**:\n",
    "  - Normalization should be applied to all features since some classifiers are sensitive to feature scaling.\n",
    "- **Documentation**: Record the decision and reasoning behind normalization.\n",
    "\n",
    "### 2. Handling Class Imbalance\n",
    "- **Techniques to Address Imbalance**:\n",
    "  1. **SMOTE (Synthetic Minority Oversampling Technique)**: Generate synthetic samples for the minority class.\n",
    "  2. **Class Weights**: Assign higher weights to the minority class during training.\n",
    "  3. **Ensemble Methods**: Use techniques like Random Forest or Bagging that are robust to imbalanced data.\n",
    "- **Classifier Dependency**:\n",
    "  - Some imbalance techniques are suitable only for specific classification algorithms.\n",
    "  - **Documentation**: Clearly note the classifiers compatible with each imbalance-handling technique.\n",
    "\n",
    "---\n",
    "\n",
    "## Notes\n",
    "- Keep a record of all decisions and approaches in the notebook or Markdown.\n",
    "- Justify each step with reasoning or research findings for transparency and reproducibility.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Data Imbalance Techniques\n",
    "\n",
    "## 1. SMOTE (Synthetic Minority Oversampling Technique)\n",
    "- **Purpose**: Generates synthetic samples for the minority class to balance the dataset.\n",
    "- **How it Works**: SMOTE creates new samples by interpolating between existing minority class examples.\n",
    "- **Extensions**:\n",
    "  - Borderline SMOTE: Focuses on samples near the decision boundary.\n",
    "  - ADASYN: Generates more synthetic samples for harder-to-learn minority examples.\n",
    "  - SMOTETomek: Combines SMOTE with Tomek Links to clean the dataset.\n",
    "- **Resources**:\n",
    "  - [SMOTE for Imbalanced Classification](https://www.geeksforgeeks.org/smote-for-imbalanced-classification-with-python/): Includes table for when to use each variant.\n",
    "  - [imblearn.over_sampling.SMOTE Documentation](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Class Weights\n",
    "- **Purpose**: Adjust model training to account for class imbalance.\n",
    "- **How it Works**: Assigns higher weights to the minority class, forcing the model to focus on it more.\n",
    "- **F1 Score Formula**:\n",
    "  \\[ F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\]\n",
    "  - If F1 = 0, the model performs poorly on the minority class.\n",
    "- **Implementation**:\n",
    "  - Use the `class_weight` parameter in classifiers such as Scikit-learn, LightGBM, or CatBoost.\n",
    "    - Example: For Logistic Regression, set `class_weight='balanced'` or provide manual weights.\n",
    "- **Resources**:\n",
    "  - [Improve Class Imbalance Using Class Weights](https://www.analyticsvidhya.com/blog/2020/10/improve-class-imbalance-class-weights/).\n",
    "  - [How to Set Class Weights in Keras](https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Ensemble Methods\n",
    "- **Purpose**: Combines multiple classifiers to improve performance and handle class imbalance.\n",
    "- **Techniques**:\n",
    "  - **Data-Level Approaches**:\n",
    "    - **Undersampling**: Reduces the majority class size.\n",
    "    - **Oversampling**: Increases the minority class size.\n",
    "    - **Hybrid Approaches**: Combines under and oversampling methods.\n",
    "  - **Algorithm-Level Techniques**:\n",
    "    - **Cost-Sensitive Learning**: Assigns different misclassification costs to classes.\n",
    "    - **Threshold-Moving**: Adjusts the decision threshold to favor the minority class.\n",
    "  - **Ensemble Learning Methods**:\n",
    "    - **Bagging**: SMOTEBagging – Combines SMOTE with bagging methods.\n",
    "    - **Boosting**: RUSBoost – Applies random undersampling with boosting.\n",
    "    - **Stacking**: EasyEnsemble – Combines multiple models with data resampling.\n",
    "    - **Hybrid Methods**: Mix bagging + boosting, hybrid sampling + ensemble learning, or dynamic selection + preprocessing.\n",
    "- **Resources**:\n",
    "  - [Ensemble Techniques for Class Imbalance](https://thecontentfarm.net/ensemble-techniques-for-handling-class-imbalance/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Value Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values:\n",
      "max_glu_serum    96420\n",
      "A1Cresult        84748\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_values = data.isnull().sum()\n",
    "columns_with_missing_values = missing_values[missing_values > 0]\n",
    "print(f\"Columns with missing values:\\n{columns_with_missing_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is observed from the dataset that in some columns, missing values are represented by the '?' character. Hence, a second data exploration step for analyzing which columns contain the question mark character and their counts is conducted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns containing question marks and their counts:\n",
      "race                  2273\n",
      "weight               98569\n",
      "payer_code           40256\n",
      "medical_specialty    49949\n",
      "diag_1                  21\n",
      "diag_2                 358\n",
      "diag_3                1423\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "question_marks = (data == '?').sum()\n",
    "columns_with_question_marks = question_marks[question_marks > 0]\n",
    "print(f\"Columns containing question marks and their counts:\\n{columns_with_question_marks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this observation, we decided to find the summation of missing values (both NaN and ? characters). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values (including '?' and NaN):\n",
      "race                  2273\n",
      "weight               98569\n",
      "payer_code           40256\n",
      "medical_specialty    49949\n",
      "diag_1                  21\n",
      "diag_2                 358\n",
      "diag_3                1423\n",
      "max_glu_serum        96420\n",
      "A1Cresult            84748\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "total_missing_values = missing_values + question_marks\n",
    "columns_with_all_missing_values = total_missing_values[total_missing_values > 0]\n",
    "print(\"Columns with missing values (including '?' and NaN):\")\n",
    "print(columns_with_all_missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values:\n",
      "race                  2273\n",
      "weight               98569\n",
      "payer_code           40256\n",
      "medical_specialty    49949\n",
      "diag_1                  21\n",
      "diag_2                 358\n",
      "diag_3                1423\n",
      "max_glu_serum        96420\n",
      "A1Cresult            84748\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert '?'s into pandas NA values\n",
    "data = data.replace('?', pd.NA)\n",
    "\n",
    "# Check to see if the missing value summations match with the previous cell's output\n",
    "missing_values = data.isnull().sum()\n",
    "columns_with_missing_values = missing_values[missing_values > 0]\n",
    "print(f\"Columns with missing values:\\n{columns_with_missing_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Approach: Deleting the Rows with Missing Values \n",
    "\n",
    "Remove rows that contain at least one missing value in any column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (101766, 50)\n",
      "Cleaned dataset shape: (0, 50)\n"
     ]
    }
   ],
   "source": [
    "data_dropped = data.dropna()\n",
    "print(f\"Original dataset shape: {data.shape}\")\n",
    "print(f\"Cleaned dataset shape: {data_dropped.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is observed from the output of the previous cell that no rows remain in the dataset when we drop the rows containing a missing value. This suggests that each row contains at least one missing value.\n",
    "\n",
    "Hence, a more sophisticated missing-value handling is needed. \n",
    "\n",
    "Firstly, the following three columns contain approximately 90% missing values:\n",
    "- weight               (98569 missing values out of 101766)\n",
    "- max_glu_serum        (96420 missing values out of 101766)\n",
    "- A1Cresult            (84748 missing values out of 101766)\n",
    "\n",
    "Hence, they are not informative and can be dropped before continuing with the rest of the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (101766, 50)\n",
      "Dataset shape after dropping columns: (101766, 47)\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = ['weight', 'max_glu_serum', 'A1Cresult']\n",
    "data_columns_dropped = data.drop(columns=columns_to_drop, axis=1)\n",
    "print(f\"Original dataset shape: {data.shape}\")\n",
    "print(f\"Dataset shape after dropping columns: {data_columns_dropped.shape}\")\n",
    "\n",
    "data = data_columns_dropped.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: The two columns that contain approximately 50% missing values can also be dropped. Discuss it with your teammates.\n",
    "\n",
    "- payer_code           (40256 missing values out of 101766)\n",
    "- medical_specialty    (49949 missing values out of 101766)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (101766, 47)\n",
      "Cleaned dataset shape: (26755, 47)\n"
     ]
    }
   ],
   "source": [
    "# Now, again remove rows that contain at least one missing value in any column.\n",
    "data_dropped = data.dropna()\n",
    "print(f\"Original dataset shape: {data.shape}\")\n",
    "print(f\"Cleaned dataset shape: {data_dropped.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation-based Imputation\n",
    "\n",
    "Identify which features are highly correlated and impute missing values accordingly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
