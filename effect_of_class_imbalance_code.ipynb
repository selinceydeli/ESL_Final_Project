{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elements of Statistical Learning (ESL) Final Project\n",
    "\n",
    "**Our Machine Learning Question:**\n",
    "\n",
    "**How can we improve the accuracy of predicting** which diabetic patients are likely to be readmitted to the hospital within 30 days across different classifiers **by addressing the severe class imbalance in the dataset?**\n",
    "The dataset we chose is on “Early Readmission Prediction of Patients Diagnosed with Diabetes,” which has a significant class imbalance between early admitted patients and others. The positive class for our classification (early admitted patients) is underrepresented with a ratio of 1-to-9.\n",
    "\n",
    "The techniques we are considering implementing to overcome the class imbalance in the dataset are as follows:\n",
    "1. SMOTE (Synthetic Minority Oversampling Technique)\n",
    "2. Class weights\n",
    "3. Ensemble methods\n",
    "\n",
    "All the above techniques will be compared across different classifiers to answer the following question: How well do different models (e.g., Logistic Regression, Random Forest, SVM) handle the imbalance and predict early readmissions?\n",
    "\n",
    "Recommendations from Professor David:\n",
    "\n",
    "- Check how different models react to class imbalance prior to implementing balancing techniques.\n",
    "- Also, check if the balancing techniques actually cause that the same objects are misclassified, or that suddenly also other objects go wrong (that used to be classified well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:44:08.315155Z",
     "start_time": "2025-01-13T09:44:06.283072Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Exploration\n",
    "\n",
    "The chosen dataset represents ten years (1999-2008) of clinical care at 130 US hospitals and integrated delivery networks. Each row concerns hospital records of patients diagnosed with diabetes, who underwent laboratory, medications, and stayed up to 14 days.\n",
    "\n",
    "Link to the dataset: https://archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008\n",
    "\n",
    "**Our Aim:** Identifying patients at risk of being readmitted within 30 days of discharge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:44:08.529215Z",
     "start_time": "2025-01-13T09:44:08.315155Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('diabetic_data.csv')\n",
    "dataset.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:44:08.533687Z",
     "start_time": "2025-01-13T09:44:08.529215Z"
    }
   },
   "outputs": [],
   "source": [
    "print(dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:44:08.544795Z",
     "start_time": "2025-01-13T09:44:08.533687Z"
    }
   },
   "outputs": [],
   "source": [
    "target_distribution = dataset['readmitted'].value_counts()\n",
    "\n",
    "print(\"Distribution of 'readmitted' target column:\")\n",
    "print(target_distribution)\n",
    "\n",
    "target_percentage = dataset['readmitted'].value_counts(normalize=True) * 100\n",
    "print(\"\\nPercentage distribution of 'readmitted' target column:\")\n",
    "print(f\"{round(target_percentage, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification Formulation\n",
    "\n",
    "There are three classes in the target 'readmitted' column:\n",
    "- Class 1: <30 (early admission)\n",
    "- Class 2: NO (no admission)\n",
    "- Class 3: >30 (late admission)\n",
    "\n",
    "For the purpose of our early admission prediction problem, we will use binary classification because we are looking for a way to classify early admissions correctly in this problem. Therefore, for our problem, the \"No\" and \">30\" cases mean the same and can be combined into the same group.\n",
    "\n",
    "Hence, the **binary classification** formulation can be described as follows:\n",
    "- Class 1: <30 (early admission)\n",
    "- Class 2: NO and >30 (no admission and late admission)\n",
    "\n",
    "### Understanding the Imbalance in the Dataset\n",
    "\n",
    "After formulating the dataset as a binary classification problem, it can be seen that the combined percentage of the \"No\" and \">30\" classes is approximately 88.84%, whereas the percentage distribution of the early admission class \"<30\" is 11.16%.\n",
    "\n",
    "Hence, it can be seen that there is a severe imbalance in the dataset. The positive class for our classification (early admitted patients) is underrepresented, with a ratio of 1-to-9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO During the Christmas Break\n",
    "\n",
    "Link to the Google sheets for writing down comments on the dataset columns and rationalizing the implemented approaches:\n",
    "https://docs.google.com/spreadsheets/d/1wQvVQijqFmdOWjL4nJtSQ5hRea5tVSDQo9-br04bgew/edit?usp=sharing\n",
    "\n",
    "## Data Exploration Ideas\n",
    "\n",
    "### 1. Missing Value Handling\n",
    "- **Indicator of Missing Values**: Missing values are represented by `?` in the dataset.\n",
    "- **Imputation Strategies**:\n",
    "  - **Baseline Approach: Deleting the Rows with Missing Values:** Remove rows that contain at least one missing value in any column.\n",
    "  - **Correlation-based Imputation**: Identify which features are highly correlated and impute missing values accordingly.\n",
    "  - **Statistical Methods**:\n",
    "    - For **categorical features**: Use the most frequent class (mode).\n",
    "    - For **numerical features**: Use the mean, median, or a neighbor-based approach (e.g., K-Nearest Neighbors).\n",
    "      - **Note**: Research the name of the neighbor-based imputation technique (e.g., KNN Imputation).\n",
    "- **Documentation**: Record the rationale behind the chosen imputation strategy.\n",
    "\n",
    "### 2. Feature Reduction & Extraction\n",
    "- **Initial Cleanup**:\n",
    "  - Drop non-informative columns such as:\n",
    "    - ID fields.\n",
    "    - Columns with constant values (e.g., same value for all rows).\n",
    "- **Rationale**: Clearly document why specific columns were dropped.\n",
    "\n",
    "- **Dimensionality Reduction**:\n",
    "  - Implement methods like **Principal Component Analysis (PCA)**.\n",
    "  - **Library**: Check if Scikit-learn has built-in functions for feature extraction or reduction.\n",
    "  - Alternatively, develop custom logic for feature reduction based on feature correlation.\n",
    "- **Rationale**: Justify the use of any dimensionality reduction method and its impact on the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Preprocessing Ideas\n",
    "\n",
    "### 1. Normalization\n",
    "- **Research Questions**:\n",
    "  - Which features require normalization?\n",
    "    - Should normalization apply only to the target class or to all features?\n",
    "- **Initial Assumption**:\n",
    "  - Normalization should be applied to all features since some classifiers are sensitive to feature scaling.\n",
    "- **Documentation**: Record the decision and reasoning behind normalization.\n",
    "\n",
    "### 2. Handling Class Imbalance\n",
    "- **Techniques to Address Imbalance**:\n",
    "  1. **SMOTE (Synthetic Minority Oversampling Technique)**: Generate synthetic samples for the minority class.\n",
    "  2. **Class Weights**: Assign higher weights to the minority class during training.\n",
    "  3. **Ensemble Methods**: Use techniques like Random Forest or Bagging that are robust to imbalanced data.\n",
    "- **Classifier Dependency**:\n",
    "  - Some imbalance techniques are suitable only for specific classification algorithms.\n",
    "  - **Documentation**: Clearly note the classifiers compatible with each imbalance-handling technique.\n",
    "\n",
    "---\n",
    "\n",
    "## Notes\n",
    "- Keep a record of all decisions and approaches in the notebook or Markdown.\n",
    "- Justify each step with reasoning or research findings for transparency and reproducibility.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Data Imbalance Techniques\n",
    "\n",
    "## 1. SMOTE (Synthetic Minority Oversampling Technique)\n",
    "- **Purpose**: Generates synthetic samples for the minority class to balance the dataset.\n",
    "- **How it Works**: SMOTE creates new samples by interpolating between existing minority class examples.\n",
    "- **Extensions**:\n",
    "  - Borderline SMOTE: Focuses on samples near the decision boundary.\n",
    "  - ADASYN: Generates more synthetic samples for harder-to-learn minority examples.\n",
    "  - SMOTETomek: Combines SMOTE with Tomek Links to clean the dataset.\n",
    "- **Resources**:\n",
    "  - [SMOTE for Imbalanced Classification](https://www.geeksforgeeks.org/smote-for-imbalanced-classification-with-python/): Includes table for when to use each variant.\n",
    "  - [imblearn.over_sampling.SMOTE Documentation](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Class Weights\n",
    "- **Purpose**: Adjust model training to account for class imbalance.\n",
    "- **How it Works**: Assigns higher weights to the minority class, forcing the model to focus on it more.\n",
    "- **F1 Score Formula**:\n",
    "  \\[ F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\]\n",
    "  - If F1 = 0, the model performs poorly on the minority class.\n",
    "- **Implementation**:\n",
    "  - Use the `class_weight` parameter in classifiers such as Scikit-learn, LightGBM, or CatBoost.\n",
    "    - Example: For Logistic Regression, set `class_weight='balanced'` or provide manual weights.\n",
    "- **Resources**:\n",
    "  - [Improve Class Imbalance Using Class Weights](https://www.analyticsvidhya.com/blog/2020/10/improve-class-imbalance-class-weights/).\n",
    "  - [How to Set Class Weights in Keras](https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Ensemble Methods\n",
    "- **Purpose**: Combines multiple classifiers to improve performance and handle class imbalance.\n",
    "- **Techniques**:\n",
    "  - **Data-Level Approaches**:\n",
    "    - **Undersampling**: Reduces the majority class size.\n",
    "    - **Oversampling**: Increases the minority class size.\n",
    "    - **Hybrid Approaches**: Combines under and oversampling methods.\n",
    "  - **Algorithm-Level Techniques**:\n",
    "    - **Cost-Sensitive Learning**: Assigns different misclassification costs to classes.\n",
    "    - **Threshold-Moving**: Adjusts the decision threshold to favor the minority class.\n",
    "  - **Ensemble Learning Methods**:\n",
    "    - **Bagging**: SMOTEBagging – Combines SMOTE with bagging methods.\n",
    "    - **Boosting**: RUSBoost – Applies random undersampling with boosting.\n",
    "    - **Stacking**: EasyEnsemble – Combines multiple models with data resampling.\n",
    "    - **Hybrid Methods**: Mix bagging + boosting, hybrid sampling + ensemble learning, or dynamic selection + preprocessing.\n",
    "- **Resources**:\n",
    "  - [Ensemble Techniques for Class Imbalance](https://thecontentfarm.net/ensemble-techniques-for-handling-class-imbalance/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Dataset\n",
    "\n",
    "- Merge the \">30\" and \"NO\" categories of the target variable\n",
    "- Split the dataset into training and test sets first, ensuring that all preprocessing steps are based on the training data. This will prevent **data leakage**. In other words, information from the test set will not influence the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:44:10.788415Z",
     "start_time": "2025-01-13T09:44:08.545860Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Merging \">30\" and \"NO\" (not readmitted) categories of the target variable\n",
    "dataset['readmitted'] = dataset['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "\n",
    "dataset.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate target column\n",
    "X = dataset.drop('readmitted', axis=1) # inplace=False (default)\n",
    "y = dataset['readmitted']\n",
    "\n",
    "# Splitting the dataset in training and test sets\n",
    "# From this point on, we will only use the training set until we test the models\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# TODO grouping? to prevent data leakage\n",
    "# We can't have information of the same person being separated in training and test sets\n",
    "\n",
    "\"\"\"\n",
    "# Scaling\n",
    "# Scaling could only be implemented after we convert the categorical variables into numeric.\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)  # Fit on training data only\n",
    "X_train = pd.DataFrame(scaler.transform(X_train), columns=X.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns=X.columns)\n",
    "\"\"\"\n",
    "\n",
    "data = pd.concat([X_train.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1) # Training dataset\n",
    "test_data = pd.concat([X_test.reset_index(drop=True), y_test.reset_index(drop=True)], axis=1) # Test dataset\n",
    "\n",
    "print(\"Shape of the original dataset:\", dataset.shape)\n",
    "print(\"Shape of the training dataset (data):\", data.shape)\n",
    "print(\"Shape of the test dataset (test_data):\", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Value Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:44:10.844614Z",
     "start_time": "2025-01-13T09:44:10.788415Z"
    }
   },
   "outputs": [],
   "source": [
    "missing_values = data.isnull().sum()\n",
    "columns_with_missing_values = missing_values[missing_values > 0]\n",
    "print(f\"Columns with missing values:\\n{columns_with_missing_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is observed from the dataset that in some columns, missing values are represented by the '?' character. Hence, a second data exploration step for analyzing which columns contain the question mark character and their counts is conducted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:44:10.926054Z",
     "start_time": "2025-01-13T09:44:10.844614Z"
    }
   },
   "outputs": [],
   "source": [
    "question_marks = (data == '?').sum()\n",
    "columns_with_question_marks = question_marks[question_marks > 0]\n",
    "print(f\"Columns containing question marks and their counts:\\n{columns_with_question_marks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this observation, we decided to find the summation of missing values (both NaN and ? characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:44:10.929620Z",
     "start_time": "2025-01-13T09:44:10.926054Z"
    }
   },
   "outputs": [],
   "source": [
    "total_missing_values = missing_values + question_marks\n",
    "columns_with_all_missing_values = total_missing_values[total_missing_values > 0]\n",
    "print(\"Columns with missing values (including '?' and NaN):\")\n",
    "print(columns_with_all_missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:44:11.074719Z",
     "start_time": "2025-01-13T09:44:10.929620Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert '?'s into pandas NA values\n",
    "data = data.replace('?', pd.NA)\n",
    "test_data = test_data.replace('?', pd.NA)\n",
    "\n",
    "# Check to see if the missing value summations match with the previous cell's output\n",
    "missing_values = data.isnull().sum()\n",
    "columns_with_missing_values = missing_values[missing_values > 0]\n",
    "print(f\"Columns with missing values:\\n{columns_with_missing_values}\")\n",
    "\n",
    "assert (total_missing_values == missing_values).all(), \"Mismatch in missing value summations!\"\n",
    "\n",
    "# If the assertion is successful, print a success message\n",
    "print(\"\\nConsistency check passed: Missing value summations match for '?' and NaN handling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Approach: Deleting the Rows with Missing Values\n",
    "\n",
    "Remove rows that contain at least one missing value in any column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:44:11.122682Z",
     "start_time": "2025-01-13T09:44:11.074719Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dropped = data.dropna()\n",
    "print(f\"Original dataset shape: {data.shape}\")\n",
    "print(f\"Cleaned dataset shape after dropping rows: {data_dropped.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is observed from the output of the previous cell that no rows remain in the dataset when we drop the rows containing a missing value. This suggests that each row contains at least one missing value.\n",
    "\n",
    "Hence, a more sophisticated missing-value handling is needed.\n",
    "\n",
    "Firstly, the following three columns contain between approximately 83% - 97% missing values:\n",
    "- weight               (78916 missing values out of 81412)\n",
    "- max_glu_serum        (77130 missing values out of 81412)\n",
    "- A1Cresult            (67793 missing values out of 81412)\n",
    "\n",
    "Hence, they are not informative and can be dropped before continuing with the rest of the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:44:11.151839Z",
     "start_time": "2025-01-13T09:44:11.122682Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Original training dataset shape: {data.shape}\")\n",
    "columns_to_drop = ['weight', 'max_glu_serum', 'A1Cresult']\n",
    "data = data.drop(columns=columns_to_drop, axis=1)\n",
    "print(f\"Training dataset shape after dropping columns: {data.shape}\")\n",
    "\n",
    "# Drop the same columns from the test data as well\n",
    "print(f\"Original test dataset shape: {test_data.shape}\")\n",
    "test_data = test_data.drop(columns=columns_to_drop, axis=1)\n",
    "print(f\"Test dataset shape after dropping columns: {test_data.shape}\")\n",
    "\n",
    "# Drop the same columns from the test data as well\n",
    "print(f\"Entire dataset shape: {dataset.shape}\")\n",
    "dataset = dataset.drop(columns=columns_to_drop, axis=1)\n",
    "print(f\"Entire dataset shape after dropping columns: {dataset.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: The two columns that contain approximately 50% missing values can also be dropped. Discuss it with your teammates.\n",
    "\n",
    "- payer_code           (32157 missing values out of 81412)\n",
    "- medical_specialty    (40057 missing values out of 81412)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:44:11.205469Z",
     "start_time": "2025-01-13T09:44:11.151839Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now, again remove rows that contain at least one missing value in any column.\n",
    "print(f\"Dataset shape before dropping rows: {data.shape}\")\n",
    "data_dropped = data.dropna()\n",
    "print(f\"Cleaned dataset shape after dropping rows: {data_dropped.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "Since a significant amount (a forth) of all entries have been deleted, *Deleting the Rows with Missing Values* is not the most reasonable approach but it'll serve as a baseline for the filling of missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation-based Imputation\n",
    "\n",
    "Identify which features are highly correlated and impute missing values accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:44:11.209304Z",
     "start_time": "2025-01-13T09:44:11.205469Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Column types before applying mapping:\")\n",
    "print(data.dtypes[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:44:11.304377Z",
     "start_time": "2025-01-13T09:44:11.209304Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Column types before applying mapping:\")\n",
    "print(data.dtypes[25:])\n",
    "\n",
    "# First, apply mapping to non-numeric columns,\n",
    "# prior to computing the correlation between the features\n",
    "\n",
    "data_encoded = data.copy()\n",
    "for column in data.select_dtypes(include=['object', 'category']).columns:\n",
    "    data_encoded[column] = data_encoded[column].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:44:11.308462Z",
     "start_time": "2025-01-13T09:44:11.304377Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Column types after applying mapping:\")\n",
    "print(data_encoded.dtypes[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:44:11.312090Z",
     "start_time": "2025-01-13T09:44:11.308462Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Column types after applying mapping:\")\n",
    "print(data_encoded.dtypes[25:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:44:11.322692Z",
     "start_time": "2025-01-13T09:44:11.312090Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check the first 5 rows of the mapped data to see the encodings\n",
    "print(data_encoded.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:44:11.530058Z",
     "start_time": "2025-01-13T09:44:11.322692Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now, compute the correlation matrix using the encoded data\n",
    "\n",
    "correlation_matrix = data_encoded.corr()\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:44:11.533750Z",
     "start_time": "2025-01-13T09:44:11.530058Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_high_correlations(correlation_threshold, corr_matrix): # TODO changed the name from correlation_matrix -M\n",
    "    \"\"\"\n",
    "        Method for calculating highly correlated features above a given threshold\n",
    "    \"\"\"\n",
    "    highly_correlated_features = corr_matrix[(corr_matrix.abs() > correlation_threshold) & (corr_matrix != 1.0)]\n",
    "    print(f\"\\nHighly Correlated Features (Threshold > {correlation_threshold}):\")\n",
    "    print(highly_correlated_features.dropna(how=\"all\", axis=0).dropna(how=\"all\", axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:44:11.541068Z",
     "start_time": "2025-01-13T09:44:11.533750Z"
    }
   },
   "outputs": [],
   "source": [
    "compute_high_correlations(correlation_threshold=0.6, corr_matrix=correlation_matrix)\n",
    "compute_high_correlations(correlation_threshold=0.5, corr_matrix=correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Observation**\n",
    "\n",
    "It has been observed that the highest correlation among the features is 0.5 in magnitude (as the absolute values of the correlation matrix are used for computations). No correlations above this threshold have been observed.\n",
    "\n",
    "Since 0.5 is not considered a high correlation, implementing a correlation-based imputation technique for handling missing values does not seem plausible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chosen Imputation Approach: Statistical Methods for Missing Value Handling\n",
    "\n",
    "- Missing values in **categorical features** will be imputed using the most frequent class (mode).\n",
    "\n",
    "- Missing values in **numerical features** will be imputed using the **k-nearest neighbor (KNN) imputation approach**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:44:12.220146Z",
     "start_time": "2025-01-13T09:44:11.541068Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "print(\"Shape of training dataset before imputation:\", data.shape)\n",
    "print(\"Shape of test dataset before imputation:\", test_data.shape)\n",
    "\n",
    "# All NA values are converted into nan for compatibility with sklearn\n",
    "data_impute = data.replace({pd.NA: np.nan})\n",
    "test_data_impute = test_data.replace({pd.NA: np.nan})\n",
    "\n",
    "categorical_columns = data_impute.select_dtypes(include=['object', 'category']).columns\n",
    "numerical_columns = data_impute.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Handling missing values in categorical features using Simple Imputer with \"most frequent\" strategy\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "data_impute[categorical_columns] = categorical_imputer.fit_transform(data_impute[categorical_columns])\n",
    "test_data_impute[categorical_columns] = categorical_imputer.transform(test_data_impute[categorical_columns])\n",
    "\n",
    "# Handling missing values in numerical features using KNN Imputer with the given k value\n",
    "k = 5 # TODO test different k values with pipeline\n",
    "numerical_imputer = KNNImputer(n_neighbors=k)\n",
    "data_impute[numerical_columns] = numerical_imputer.fit_transform(data_impute[numerical_columns])\n",
    "test_data_impute[numerical_columns] = numerical_imputer.transform(test_data_impute[numerical_columns])\n",
    "\n",
    "print(\"Shape of training dataset after imputation:\", data_impute.shape)\n",
    "print(\"Shape of test dataset after imputation:\", test_data_impute.shape)\n",
    "print(\"Total missing values in training dataset after imputation:\", data_impute.isnull().sum().sum())\n",
    "print(\"Total missing values in test dataset after imputation:\", test_data_impute.isnull().sum().sum())\n",
    "\n",
    "print(\"First 20 Rows of the Imputed Training Dataset:\")\n",
    "data_impute.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:44:12.232810Z",
     "start_time": "2025-01-13T09:44:12.220146Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"First 20 Rows of the Imputed Test Dataset:\")\n",
    "test_data_impute.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Encoding \n",
    "\n",
    "Use sklearn's label encoder to convert the categorical columns into numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Apply mapping to non-numeric columns\n",
    "mapped_data = data_impute.copy() # Training data\n",
    "test_mapped_data = test_data_impute.copy() # Test data\n",
    "\n",
    "for column in dataset.select_dtypes(include=['object', 'category']).columns:\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    trained_le = label_encoder.fit(dataset[column]) # Using the original dataset to correctly encapture all unique categorical values \n",
    "    mapped_data[column] = trained_le.transform(mapped_data[column])\n",
    "    test_mapped_data[column] = trained_le.transform(test_mapped_data[column])\n",
    "\n",
    "mapped_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mapped_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Reduction & Extraction Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Feature Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Variance Threshold Feature Reduction Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "X_train_mapped = mapped_data.drop(columns=['readmitted'])\n",
    "\n",
    "selector = VarianceThreshold(threshold=0)\n",
    "\n",
    "selector.fit(X_train_mapped)\n",
    "selector.get_support()\n",
    "\n",
    "cols = [column for column in X_train_mapped.columns\n",
    "          if column not in X_train_mapped.columns[selector.get_support()]]\n",
    "\n",
    "print(\"Columns removed due to having 0 variance:\")\n",
    "print(cols)\n",
    "\n",
    "print(\"Shape of training dataset before dropping 0 variance columns:\", mapped_data.shape)\n",
    "print(\"Shape of test dataset before dropping 0 variance columns::\", test_mapped_data.shape)\n",
    "\n",
    "mapped_data.drop(cols, axis=1, inplace=True)\n",
    "test_mapped_data.drop(cols, axis=1, inplace=True)\n",
    "\n",
    "print(\"Shape of training dataset after dropping 0 variance columns:\", mapped_data.shape)\n",
    "print(\"Shape of test dataset after dropping 0 variance columns::\", test_mapped_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation-based Feature Reduction Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the correlation matrix on the mapped training dataset\n",
    "corr_matrix_mapped = mapped_data.corr()\n",
    "print(\"Correlation Matrix for Mapped Data:\")\n",
    "print(corr_matrix_mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:44:12.561508Z",
     "start_time": "2025-01-13T09:44:12.232810Z"
    }
   },
   "outputs": [],
   "source": [
    "compute_high_correlations(correlation_threshold=0.6, corr_matrix=corr_matrix_mapped)\n",
    "compute_high_correlations(correlation_threshold=0.5, corr_matrix=corr_matrix_mapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "\n",
    "The highest correlation among the features is 0.5 in absolute value. Since 0.5 is not considered a high correlation, implementing a correlation-based feature reduction technique on the imputated data does not seem plausible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_high_corr_features(data_set, threshold):\n",
    "    col_corr = set() # Set of all the names of deleted columns\n",
    "    corr_mtrx = data_set.corr()\n",
    "    # pd.DataFrame(data={'c0': [1, 2, 3], 'c1': [4, 5, 6], 'c2': [7, 8, 9]})\n",
    "\n",
    "    for i in range(1, len(corr_mtrx.columns)):\n",
    "        for j in range(0, i):\n",
    "            # print(\"(\", i, \", \", j, \"):\", corr_mtrx.iloc[i, j])\n",
    "            # print(corr_mtrx.columns[j])\n",
    "\n",
    "            if (abs(corr_mtrx.iloc[i, j]) >= threshold) and (corr_mtrx.columns[j] not in col_corr):\n",
    "                col_name = corr_mtrx.columns[j] # getting the name of column\n",
    "                col_corr.add(col_name)\n",
    "                # print(col_corr) # -> columns that are removed due to having above the threshold absolute correlation\n",
    "\n",
    "                if col_name in data_set.columns:\n",
    "                    data_set.drop(columns=col_name, inplace=True)\n",
    "                    # print(data_set)\n",
    "                    # print(corr_mtrx) # -> dataset's correlation matrix\n",
    "\n",
    "    print(\"Deleted\", len(col_corr), \"columns with threshold\", threshold)\n",
    "    print(col_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mapped = mapped_data.drop(columns=['readmitted'])\n",
    "\n",
    "remove_high_corr_features(X_train_mapped, 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Forward Selection (Wrapper Approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Use mapped_data and test_mapped_data datasets\n",
    "# The current number of features is 43!\n",
    "\n",
    "X_train = mapped_data.drop(columns=['readmitted'])\n",
    "y_train = mapped_data['readmitted']\n",
    "\n",
    "X_test = test_mapped_data.drop(columns=['readmitted'])\n",
    "y_test = test_mapped_data['readmitted']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Testing the possible values for k_features \n",
    "# For forward feature selection\n",
    "k_features = [5, 8, 12, 15, 18, 20]\n",
    "\n",
    "best_k = 0\n",
    "best_val_score = 0 \n",
    "\n",
    "for k in k_features:\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    ffs = sfs(clf, k_features=k, forward=True, floating=False, verbose=0, scoring='f1', cv=5) # Use a cross-validation of 5 folds\n",
    "\n",
    "    ffs.fit(X_train, y_train)\n",
    "    selected_features = list(ffs.k_feature_names_)  # Selected features\n",
    "    print(f\"Selected {k} features: {selected_features}\")\n",
    "\n",
    "    # Use selected features to fit a logistic classifier and evaluate on validation set\n",
    "    clf.fit(X_train[selected_features], y_train)\n",
    "    val_score = clf.score(X_val[selected_features], y_val)  \n",
    "    \n",
    "    print(f\"Validation score for {k} features: {val_score}\")\n",
    "\n",
    "    if val_score > best_val_score:\n",
    "        best_k = k\n",
    "        best_val_score = val_score\n",
    "\n",
    "print(f\"\\nBest number of features: {best_k} with Validation score: {best_val_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT TESTED\n",
    "\n",
    "# TODO model?\n",
    "lreg = LinearRegression()\n",
    "ffs = sfs(lreg, k_features=20, forward=True, floating=False, verbose=2, scoring='f1')\n",
    "# TODO select how many features? (1, 43?)\n",
    "# verbose=2 to show results while processing, see where metric stops improving\n",
    "# TODO add cross validation?\n",
    "# TODO best scoring metric?\n",
    "\n",
    "X_fs = mapped_data.drop(columns=['readmitted'])\n",
    "y_fs = mapped_data['readmitted']\n",
    "\n",
    "ffs = ffs.fit(X_fs, y_fs)\n",
    "feat_names = list(ffs.k_feature_names_) # selected features\n",
    "print(feat_names)\n",
    "print(ffs.k_score_) # prediction score of the selected features\n",
    "\n",
    "# print(ffs.subsets_)\n",
    "# print('Best subset (indices):', ffs.k_feature_idx_)\n",
    "# print('Best subset (corresponding names):', ffs.k_feature_names_)\n",
    "\n",
    "# (pd.DataFrame.from_dict(ffs.get_metric_dict()).T)\n",
    "\n",
    "# resulting dataset\n",
    "new_data = mapped_data[feat_names]\n",
    "new_data['readmitted'] = mapped_data['readmitted']\n",
    "print(\"before:\",mapped_data.shape, \"and after:\", new_data.shape, \"Forward Selection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# NOT TESTED\n",
    "# scaled_data = preprocessing.scale(data.T)\n",
    "# StandardScaler().fit_transform(data.T)\n",
    "\n",
    "X_train_pca = mapped_data.drop(columns=['readmitted'])\n",
    "y_train_pca = mapped_data['readmitted']\n",
    "\n",
    "X_test_pca = test_mapped_data.drop(columns=['readmitted'])\n",
    "y_test_pca = test_mapped_data['readmitted']\n",
    "\n",
    "# scikit-learn chooses the minimum number of principal components such that 95 percent of the variance is retained\n",
    "pca = PCA(.95)\n",
    "pca.fit(X_train_pca)\n",
    "\n",
    "X_train_pca = pca.transform(X_train_pca)\n",
    "X_test_pca = pca.transform(X_test_pca)\n",
    "\n",
    "# graphs?\n",
    "\n",
    "# logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding\n",
    "\n",
    "Encode non-numerical features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:44:16.399247Z",
     "start_time": "2025-01-13T09:44:12.564698Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "data_onehot = data_impute.copy() # Training dataset used for all the preprocessing\n",
    "\n",
    "# this is already done above\n",
    "# data_onehot['binary'] = data_onehot['readmitted'].apply(lambda x: 1 if x == '<30' else 0)\n",
    "# data_onehot.drop(['readmitted'], axis=1, inplace=True)\n",
    "\n",
    "categorical_features = data_onehot.select_dtypes(include=['object', 'category']).columns\n",
    "numerical_features = data_onehot.select_dtypes(include=['number']).columns\n",
    "\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "encoder.fit(data_onehot[categorical_features])\n",
    "\n",
    "encoded_train = encoder.transform(data_onehot[categorical_features])\n",
    "encoded_train_df = pd.DataFrame(encoded_train, columns=encoder.get_feature_names_out(categorical_features))\n",
    "\n",
    "encoded_test = encoder.transform(test_data_impute[categorical_features])\n",
    "encoded_test_df = pd.DataFrame(encoded_test, columns=encoder.get_feature_names_out(categorical_features))\n",
    "\n",
    "data_onehot = pd.concat([data_onehot.reset_index(drop=True).drop(columns=categorical_features), encoded_train_df], axis=1)\n",
    "test_onehot = pd.concat([test_data_impute.reset_index(drop=True).drop(columns=categorical_features), encoded_test_df], axis=1)\n",
    "\n",
    "print(f\"Training Dataset Shape Before Encoding: {data_impute.shape}\")\n",
    "print(f\"Training Dataset Shape After Encoding: {data_onehot.shape}\")\n",
    "print(data_onehot.head())\n",
    "\n",
    "print(f\"Test Dataset Shape Before Encoding: {test_data_impute.shape}\")\n",
    "print(f\"Test Dataset Shape After Encoding: {test_onehot.shape}\")\n",
    "print(test_onehot.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onehot encoding significantly increases number of features, may need feature reduction here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Reduction Implemented on One-Hot Encoded Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, implement variance threshold approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T09:50:29.944090Z",
     "start_time": "2025-01-13T09:50:27.258041Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "X_train_onehot = data_onehot.drop(columns=['readmitted'])\n",
    "\n",
    "selector = VarianceThreshold(threshold=0)\n",
    "\n",
    "selector.fit(X_train_onehot)\n",
    "selector.get_support()\n",
    "\n",
    "cols = [column for column in X_train_onehot.columns\n",
    "          if column not in X_train_onehot.columns[selector.get_support()]]\n",
    "\n",
    "print(\"Columns removed due to having 0 variance:\")\n",
    "print(cols)\n",
    "\n",
    "data_onehot.drop(cols, axis=1, inplace=True)\n",
    "test_onehot.drop(cols, axis=1, inplace=True)\n",
    "print(f\"Dataset Shape After variance drop: {data_onehot.shape}\")\n",
    "print(f\"Testset Shape After variance drop: {test_onehot.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, implement correlation-based approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T10:41:57.409294Z",
     "start_time": "2025-01-13T10:32:10.648679Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_high_corr_features2(data_set, test_set, threshold):\n",
    "    col_corr = set() # Set of all the names of deleted columns\n",
    "    corr_mtrx = data_set.corr()\n",
    "    # pd.DataFrame(data={'c0': [1, 2, 3], 'c1': [4, 5, 6], 'c2': [7, 8, 9]})\n",
    "\n",
    "    for i in range(1, len(corr_mtrx.columns)):\n",
    "        for j in range(0, i):\n",
    "            # print(\"(\", i, \", \", j, \"):\", corr_mtrx.iloc[i, j])\n",
    "            # print(corr_mtrx.columns[j])\n",
    "\n",
    "            if (abs(corr_mtrx.iloc[i, j]) >= threshold) and (corr_mtrx.columns[j] not in col_corr):\n",
    "                col_name = corr_mtrx.columns[j] # getting the name of column\n",
    "                col_corr.add(col_name)\n",
    "                # print(col_corr) # -> columns that are removed due to having above the threshold absolute correlation\n",
    "\n",
    "                if col_name in data_set.columns:\n",
    "                    data_set.drop(columns=col_name, inplace=True)\n",
    "                    test_set.drop(columns=col_name, inplace=True)\n",
    "                    # print(data_set)\n",
    "                    # print(corr_mtrx) # -> dataset's correlation matrix\n",
    "\n",
    "    print(\"Deleted columns with threshold\", threshold)\n",
    "    print(col_corr)\n",
    "    return data_set, test_set\n",
    "\n",
    "# data_onehot_corr_red = data_onehot.drop(columns=['readmitted'])\n",
    "# corr_matrix_onehot = data_onehot_corr.corr()\n",
    "# print(\"Correlation Matrix for One-Hot Encoded Data:\")\n",
    "# print(corr_matrix)\n",
    "\n",
    "print(\"Training Dataset Shape Before Correlation-based Feature Reduction:\", data_onehot.shape)\n",
    "print(\"Test Set Shape Before Correlation-based Feature Reduction:\", test_onehot.shape)\n",
    "data_onehot, test_onehot = remove_high_corr_features2(data_onehot, test_onehot, 0.9) # TODO what threshold to use?\n",
    "print(\"Training Dataset Shape After Correlation-based Feature Reduction:\", data_onehot.shape)\n",
    "print(\"Test Set Shape After Correlation-based Feature Reduction:\", test_onehot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Selection Implemented on One-Hot Encoded Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# NOT TESTED\n",
    "\n",
    "# TODO model?\n",
    "lreg = LinearRegression()\n",
    "sfs1 = sfs(lreg, k_features=4, forward=True, scoring='neg_mean_squared_error')\n",
    "# TODO select how many features?\n",
    "# \"forward\" and \"floating\" fields\n",
    "# TODO best scoring metric?\n",
    "\n",
    "# TODO change data_onehot for the one resulting from feature reduction above\n",
    "X_fs = data_onehot.drop(columns=['readmitted'])\n",
    "y_fs = data_onehot['readmitted']\n",
    "\n",
    "sfs1 = sfs1.fit(X_fs, y_fs)\n",
    "feat_names = list(sfs1.k_feature_names_) # selected features\n",
    "print(feat_names)\n",
    "\n",
    "# print(sfs1.subsets_)\n",
    "# print(sfs1.k_score_) # prediction score of the selected features\n",
    "# print('Best subset (indices):', sfs1.k_feature_idx_)\n",
    "# print('Best subset (corresponding names):', sfs1.k_feature_names_)\n",
    "\n",
    "# resulting dataset\n",
    "new_data = data_onehot[feat_names]\n",
    "new_data['readmitted'] = data_onehot['readmitted']\n",
    "print(\"before:\",data_onehot.shape, \"and after:\", new_data.shape, \"Forward Selection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction Implemented on One-Hot Encoded Dataset\n",
    "\n",
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T11:44:22.182070Z",
     "start_time": "2025-01-13T11:43:29.831952Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# NOT TESTED\n",
    "\n",
    "X_train_pca = data_onehot.drop(columns=['readmitted'])\n",
    "y_train_pca = data_onehot['readmitted']\n",
    "\n",
    "X_test_pca = test_onehot.drop(columns=['readmitted'])\n",
    "y_test_pca = test_onehot['readmitted']\n",
    "\n",
    "# scikit-learn chooses the minimum number of principal components such that 95 percent of the variance is retained\n",
    "pca = PCA(.95)\n",
    "pca.fit(X_train_pca)\n",
    "\n",
    "X_train_pca = pca.transform(X_train_pca)\n",
    "X_test_pca = pca.transform(X_test_pca)\n",
    "# print(\"Dataset X Shape After PCA:\", X_train_pca.shape)\n",
    "# print(\"Testset X Shape After PCA:\", X_train_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalance Handling \n",
    "\n",
    "#### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T10:54:09.880430Z",
     "start_time": "2025-01-13T10:54:02.543452Z"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "# Changed it to reflect the splitting in the beginning\n",
    "# Used data_onehot and test_onehot for the training and test datasets respectively\n",
    "\n",
    "X_train = data_onehot.drop(columns=['readmitted'])\n",
    "y_train = data_onehot['readmitted']\n",
    "\n",
    "X_test = test_onehot.drop(columns=['readmitted'])\n",
    "y_test = test_onehot['readmitted']\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print('Original Class Distribution:', Counter(y_train))\n",
    "print('Over-sampled Class Distribution:', Counter(y_resampled))\n",
    "\n",
    "# Important Note: The test dataset should reflect the actual class distribution in the population, although it is imbalanced.\n",
    "# Hence, we don't implement SMOTE technique on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes are now balanced using the SMOTE technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T11:13:46.215349Z",
     "start_time": "2025-01-13T11:13:45.626768Z"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "under_sampler = RandomUnderSampler(random_state=42)\n",
    "\n",
    "X_under, y_under = under_sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "print('Original Class Distribution:', Counter(y_train))\n",
    "print('Under-sampled Class Distribution:', Counter(y_under))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T11:17:50.088890Z",
     "start_time": "2025-01-13T11:17:42.556203Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "model = LogisticRegression(random_state=42, max_iter=500)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print('\\nPerformance on Original Data:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('Confusion Matrix (Original Data):\\n', confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Implement balanced class weights\n",
    "# with the parameter class_weight = \"balanced\"\n",
    "model_original = LogisticRegression(random_state=42, class_weight='balanced', max_iter=500)\n",
    "model_original.fit(X_train, y_train)\n",
    "y_pred_original = model_original.predict(X_test)\n",
    "\n",
    "print('\\nPerformance on Original Data with Balanced Class Weights:')\n",
    "print(classification_report(y_test, y_pred_original))\n",
    "print('Confusion Matrix (Original Data) with Balanced Class Weights:\\n', confusion_matrix(y_test, y_pred_original))\n",
    "\n",
    "model_smote = LogisticRegression(random_state=42, max_iter=500)\n",
    "model_smote.fit(X_resampled, y_resampled)\n",
    "y_pred_smote = model_smote.predict(X_test)\n",
    "\n",
    "print('\\nPerformance on SMOTE-Resampled Data:')\n",
    "print(classification_report(y_test, y_pred_smote))\n",
    "print('Confusion Matrix (SMOTE Data):\\n', confusion_matrix(y_test, y_pred_smote))\n",
    "\n",
    "model_under = LogisticRegression(random_state=42, max_iter=500)\n",
    "model_under.fit(X_under, y_under)\n",
    "y_pred_under = model_under.predict(X_test)\n",
    "\n",
    "print('\\nPerformance on Under-sampled Data:')\n",
    "print(classification_report(y_test, y_pred_under))\n",
    "print('Confusion Matrix (Under-sampled Data):\\n', confusion_matrix(y_test, y_pred_under))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most brute force oversampling, it somewhat increases performance on the minority class (not much though), but it also destroys predictions on the majority class, need more experiments with other methods.\n",
    "\n",
    "Undersampling and class weights all yield similar results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
